# IGC Eureka Phase 3 – Speech-Based ADRD Detection

This repository contains all scripts, and data workflows used in Phase 3 of the IGC Pharma submission to the PREPARE Challenge. Our project focuses on the detection of Alzheimer's Disease and Related Dementias (ADRD) using speech-based models, with a particular emphasis on model generalization, interpretability, and multilingual inclusion.

## Repository Structure

```
igc_phase3/
├── data/                  
│   ├── raw/               # Original downloaded datasets
│   ├── interim/           # Intermediate processing results
│   └── processed/         # Cleaned and final datasets used for training
│
├── notebooks/             # Jupyter notebooks for exploration and evaluation
│
├── src
│   └── data/             
│       ├── 01_raw_data.py                  # Script to load and inspect raw audio data
│       ├── 02_split_data.py                # Train/test split strategy, including benchmark splits
│       ├── 03_extract_metadata.py          # Extraction of speaker age, gender, and education.
│       ├── 04_identify_tasks.py            # Identify and annotate tasks per recording
│       ├── 05_extract_acustic_features.py  # Low-level audio feature extraction
│       ├── 06_STT_nonenglish_audios.py     # Speech-to-text and translation for non-English recordings
│       └── 07_TTS_nonenglish_audios.py     # Speech synthesis to English
│        
│   └── model/
├── LICENSE                 
└── README.md     
```
## Accessing the Data

Access to DementiaBank data requires authentication via a session cookie. To download data using ```src/data/01_raw_data.py```, follow these steps:

1. Create a free account at TalkBank.
2. Request access to the specific DementiaBank studies you intend to use.
3. Log in at https://talkbank.org/dementia/.
4. Open your browser’s developer tools (e.g., right-click → “Inspect” or press F12), then:
    * Go to the Network tab
    * Refresh the page
    * Click on the isLoggedIn request
    * Under the Cookies section, locate and copy the value of the talkbank cookie.

5. Run the script with the cookie:
```
python src/data/01_raw_data.py --cookie "your_cookie_value_here"
```
Downloading the full DementiaBank dataset requires approximately 140 GB of free disk space. Ensure sufficient storage is available before running ```src/data/01_raw_data.py```.

### Note

All data used in this project was obtained from the DementiaBank collection within the TalkBank repository. The data partitioning described here is based on the dataset as it existed on *May 9, 2025*. Any additions or changes to the dataset made after this date are not reflected in the current splits used for the challenge.

If needed, the partitioning process can be reproduced or updated using the script located at:
src/data/02_split_data.py

### Metadata files

Metadata for most studies is included within the downloaded transcripts. However, for some studies, **specifically Ivanova, Hopkins, and WLS**, the demographic metadata is not embedded and must be downloaded separately from their respective pages on the DementiaBank website.

To complete the dataset:
1. Visit the DementiaBank pages for the Ivanova and Hopkins studies.
2. Download the corresponding demographics files.
3. Move the downloaded files to the following directory:
```
/data/metadata/
```

Ensure the filenames match the expected format used in the pipeline.

##  Hugging Face Authentication for ```src/data/06_STT_nonenglish_audios.py```.

To run ```src/data/06_STT_nonenglish_audios.py```, you must authenticate with a valid Hugging Face token that has access to the ```meta-llama/Llama-3.2-3B-Instruct``` model.
Steps:
1. Log in or create an account at huggingface.co.
2. Accept the model license.
3. Generate a user access token
4. Add the token to your environment

### Note 1

After generating translations using ```src/data/06_STT_nonenglish_audios.py```, we manually reviewed and cleaned the output text to retain only the content that corresponded directly to the audio translation. Additional text generated by the language model, such as explanatory notes, metadata, or unsolicited elaborations, was removed. Due to the non-deterministic nature of large language models, the exact output may vary each time the script is run. As a result, we do not provide a fixed postprocessing script, since it would be tightly coupled to the specific translation outputs we received.

**Important:** Before proceeding to ```src/data/07_TTS_nonenglish_audios.py```, we recommend preprocessing the translated text to ensure that only faithful translations of the original audio are retained.

### Note 2
Transcriptions where obatined using a **ADD GCP LARGE GPU**.

## Generation of Translated Audios
To run ```src/data/07_TTS_nonenglish_audios.py```, you must first download the **OpenVoice v2** checkpoint from the official repository ([OpenVoice Usage](https://github.com/myshell-ai/OpenVoice/blob/main/docs/USAGE.md)). Ensure the checkpoint files are placed in the expected directory path as required by the script.

## Environment Setup
We recommend using two separate Python environments for this project:
* One for Whisper-related scripts (speech-to-text and translation)
* One for OpenVoice 2 (text-to-speech audio generation)

This separation is advised because of dependency incompatibilities between Whisper and OpenVoice 2. Installing both in the same environment may cause conflicts.